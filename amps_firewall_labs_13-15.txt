amps_firewall_lab_13.py
New
+9
-0

"""Compatibility wrapper around the package CLI entry point."""

from __future__ import annotations

from amps_firewall_lab_13.cli import main


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
amps_firewall_lab_13/__init__.py
New
+6
-0

"""AMPS firewall lab 13 package exposing subsystem simulators."""

from .cli import main
from .config import CONFIGURATION_KEY

__all__ = ["main", "CONFIGURATION_KEY"]
amps_firewall_lab_13/cli.py
New
+36
-0

"""Command line interface for the AMPS firewall lab 13 pipeline."""

from __future__ import annotations

import argparse
from typing import Any, Dict

from .config import CONFIGURATION_KEY
from .pipeline import run_holographic_qes_pipeline


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Run the AMPS firewall lab 13 holographic simulator")
    parser.add_argument("--mode", default=CONFIGURATION_KEY["MODE"], help="Execution mode")
    parser.add_argument("--spatial-mode", choices=["1D", "3D"], default=CONFIGURATION_KEY["GEOMETRY"]["spatial_mode"], help="Spatial simulation mode")
    parser.add_argument("--field-type", choices=["scalar", "spinor", "gauge"], default=CONFIGURATION_KEY["FIELDS"]["field_type"], help="Field type for island dynamics")
    return parser


def main(argv: list[str] | None = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)

    config = CONFIGURATION_KEY.copy()
    config["MODE"] = args.mode
    config["GEOMETRY"] = dict(CONFIGURATION_KEY["GEOMETRY"])
    config["FIELDS"] = dict(CONFIGURATION_KEY["FIELDS"])
    config["GEOMETRY"]["spatial_mode"] = args.spatial_mode
    config["FIELDS"]["field_type"] = args.field_type

    run_holographic_qes_pipeline(config)
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
amps_firewall_lab_13/config.py
New
+78
-0

"""Configuration and shared constants for the AMPS firewall lab 13 package."""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict

import numpy as np


@dataclass(frozen=True)
class OutputLayout:
    """Paths used by the holographic RG simulation pipeline."""

    root: Path = Path("outputs/amps_firewall_lab_13/holographic_island_rg")

    @property
    def geometry(self) -> Path:
        return self.root / "geometry_flow"

    @property
    def operator(self) -> Path:
        return self.root / "operator_encoding"

    @property
    def islands(self) -> Path:
        return self.root / "island_dynamics"

    @property
    def modular(self) -> Path:
        return self.root / "modular_tests"

    @property
    def dashboard(self) -> Path:
        return self.root


OUTPUT_LAYOUT = OutputLayout()

CONFIGURATION_KEY: Dict[str, Any] = {
    "MODE": "holographic_island_rg",
    "GEOMETRY": {
        "lambda_scale": 1.312_249,
        "resolution": 48,
        "planck_cutoff_m": 1.616_255e-35,
        "time_steps": 48,
        "spatial_mode": "3D",  # switchable between "1D" and "3D"
        "anisotropy_strength": 0.08,
    },
    "FIELDS": {
        "field_type": "gauge",  # scalar | spinor | gauge
        "spinor_mass": 0.37,
        "gauge_coupling": 0.12,
    },
    "RG_FLOW": {
        "initial_layers": 4,
        "max_layers": 7,
        "flow_type": "mean_curvature",
        "perturbation_strength": 0.01,
        "dynamic_layer_rate": 0.18,
        "layer_entropy_threshold": 0.12,
    },
    "HOLOGRAPHY": {
        "ads_radius_m": 9.5e-4,
        "cft_temperature_K": 2.5e-3,
        "alpha_temperature_coupling": 0.018,
    },
}


def normalise_time_vector(n_steps: int) -> np.ndarray:
    """Return a normalised time vector used by the simulators."""

    return np.linspace(0.0, 1.0, n_steps, endpoint=True)


__all__ = ["CONFIGURATION_KEY", "OUTPUT_LAYOUT", "normalise_time_vector"]
amps_firewall_lab_13/dashboard.py
New
+65
-0

"""Generate a static dashboard summarising key diagnostics."""

from __future__ import annotations

from pathlib import Path
from typing import Dict, List

from .config import OUTPUT_LAYOUT
from .io_utils import ensure_dir


def _table(headers: List[str], rows: List[List[str]]) -> str:
    divider = "|".join(["---"] * len(headers))
    header_line = "|" + "|".join(headers) + "|"
    row_lines = ["|" + "|".join(map(str, row)) + "|" for row in rows]
    return "\n".join([header_line, "|" + divider + "|", *row_lines])


def build_dashboard(payload: Dict[str, Dict[str, object]]) -> None:
    ensure_dir(OUTPUT_LAYOUT.dashboard)
    lines = ["# Holographic RG Dashboard", ""]

    if "islands" in payload:
        data = payload["islands"]
        lines.append("## Island topology transitions")
        lines.append(_table([
            "Metric",
            "Value",
        ], [["Transitions", data.get("num_topology_transitions", "-")], ["Critical times", ", ".join(map(str, data.get("critical_times", [])))]]))
        lines.append("")

    if "rg_layers" in payload:
        data = payload["rg_layers"]
        lines.append("## RG entropy gradients")
        lines.append(_table([
            "Statistic",
            "Value",
        ], [["Mean fidelity", f"{data.get('mean_fidelity', 0.0):.3f}"], ["Entropy std", f"{data.get('entropy_std', 0.0):.3f}"]]))
        lines.append("")

    if "operator" in payload:
        data = payload["operator"]
        lines.append("## Operator fidelity profile")
        lines.append(_table([
            "Layer",
            "Fidelity",
            "Support",
        ], [[idx, f"{fid:.3f}", sup] for idx, fid, sup in data.get("records", [])]))
        lines.append("")

    if "modular" in payload:
        data = payload["modular"]
        lines.append("## Modular Hamiltonian stability")
        lines.append(_table([
            "Metric",
            "Value",
        ], [["Mismatch", f"{data.get('mismatch_norm', 0.0):.4f}"], ["Eigenvalue shift std", f"{data.get('eigenvalue_shift_std', 0.0):.4f}"]]))
        lines.append("")

    report_path = OUTPUT_LAYOUT.dashboard / "dashboard.md"
    with report_path.open("w", encoding="utf-8") as handle:
        handle.write("\n".join(lines))


__all__ = ["build_dashboard"]
amps_firewall_lab_13/geometry.py
New
+130
-0

"""Geometry flow simulator with 3D spatial lattice support."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Iterable, List, Tuple

import matplotlib.pyplot as plt
import numpy as np

from .config import OUTPUT_LAYOUT, normalise_time_vector
from .io_utils import write_gif, write_json, write_npz, write_png


@dataclass
class GeometryResult:
    time: np.ndarray
    surfaces: np.ndarray
    inner_radius: np.ndarray
    outer_radius: np.ndarray
    curvature_metrics: Dict[str, float]
    frames: List[np.ndarray]


class GeometryFlowSimulator:
    """Simulate curvature flow of extremal surfaces on a 3D grid."""

    def __init__(self, config: Dict[str, float]) -> None:
        self.config = config
        self.time = normalise_time_vector(config["time_steps"])
        self._rng = np.random.default_rng(seed=42)

    def _generate_grid(self) -> Tuple[np.ndarray, float]:
        resolution = int(self.config["resolution"])
        spatial_mode = self.config.get("spatial_mode", "3D")
        if spatial_mode == "1D":
            r = np.linspace(0.05, 1.0, resolution)
            x, y, z = np.meshgrid(r, np.zeros(1), np.zeros(1), indexing="ij")
            grid = np.stack((x, y, z), axis=-1)
            max_radius = float(r[-1])
            return grid, max_radius
        extent = np.linspace(-1.0, 1.0, resolution)
        grid = np.stack(np.meshgrid(extent, extent, extent, indexing="ij"), axis=-1)
        max_radius = float(np.linalg.norm([extent[-1]] * 3))
        return grid, max_radius

    def run(self) -> GeometryResult:
        grid, max_radius = self._generate_grid()
        lambda_scale = float(self.config["lambda_scale"])
        anisotropy_strength = float(self.config.get("anisotropy_strength", 0.0))
        planck_cutoff = float(self.config["planck_cutoff_m"])

        time_steps = len(self.time)
        resolution = int(self.config["resolution"])
        spatial_mode = self.config.get("spatial_mode", "3D")

        spatial_shape = grid.shape[:-1]
        surfaces = np.zeros((time_steps, *spatial_shape), dtype=np.float32)
        inner_radius = np.zeros(time_steps)
        outer_radius = np.zeros(time_steps)
        frames: List[np.ndarray] = []

        for idx, t in enumerate(self.time):
            evap_factor = 1.0 - 0.65 * t
            base_radius = max_radius * evap_factor
            anisotropy = 1.0 + anisotropy_strength * self._rng.normal(size=grid.shape[:-1])
            radius_field = np.linalg.norm(grid, axis=-1)
            surface_field = base_radius - radius_field * anisotropy
            surface_field -= planck_cutoff * lambda_scale * t
            surfaces[idx] = surface_field

            positive_region = surface_field > 0.0
            inner_radius[idx] = np.mean(radius_field[positive_region]) if np.any(positive_region) else 0.0
            shell = np.logical_and(surface_field <= 0.0, surface_field > -0.2)
            outer_radius[idx] = np.mean(radius_field[shell]) if np.any(shell) else base_radius

            central_slice = surface_field.take(indices=surface_field.shape[-1] // 2, axis=-1)
            amplitude = np.ptp(central_slice)
            frames.append((central_slice - np.min(central_slice)) / (amplitude + 1e-9) * 255.0)

        curvature_rate = float(np.gradient(outer_radius).std())
        entropy_rate = float(np.abs(np.gradient(inner_radius)).mean())
        fixed_point = bool(entropy_rate < 1e-2)
        curvature_metrics = {
            "surface_deformation_entropy_rate": entropy_rate,
            "curvature_rate": curvature_rate,
            "fixed_point_reached": fixed_point,
            "spatial_mode": spatial_mode,
        }

        self._export_results(surfaces, inner_radius, outer_radius, frames, curvature_metrics)
        return GeometryResult(
            time=self.time,
            surfaces=surfaces,
            inner_radius=inner_radius,
            outer_radius=outer_radius,
            curvature_metrics=curvature_metrics,
            frames=frames,
        )

    def _export_results(
        self,
        surfaces: np.ndarray,
        inner_radius: np.ndarray,
        outer_radius: np.ndarray,
        frames: Iterable[np.ndarray],
        curvature_metrics: Dict[str, float],
    ) -> None:
        write_npz(
            OUTPUT_LAYOUT.geometry / "flowed_surfaces.npz",
            time=self.time,
            surfaces=surfaces,
            inner_radius=inner_radius,
            outer_radius=outer_radius,
        )
        write_json(OUTPUT_LAYOUT.geometry / "curvature_flow_metrics.json", curvature_metrics)

        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot(self.time, inner_radius, label="r_in")
        ax.plot(self.time, outer_radius, label="r_out")
        ax.set_xlabel("Time (normalised)")
        ax.set_ylabel("Radius")
        ax.set_title("Island radius evolution (reduced from 3D)")
        ax.legend()
        write_png(OUTPUT_LAYOUT.geometry / "radius_summary.png", fig)

        write_gif(OUTPUT_LAYOUT.geometry / "surface_rg_flow.gif", frames)


__all__ = ["GeometryFlowSimulator", "GeometryResult"]
amps_firewall_lab_13/io_utils.py
New
+110
-0

"""Utility helpers for file IO and figure handling."""

from __future__ import annotations

import datetime as _dt
import hashlib
import json
from pathlib import Path
from typing import Any, Dict, Iterable, List

import matplotlib.pyplot as plt
import numpy as np

try:  # pragma: no cover - optional dependency for GIF creation
    from PIL import Image
except Exception:  # pragma: no cover
    Image = None


def ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def write_json(path: Path, payload: Dict[str, Any]) -> None:
    ensure_dir(path.parent)
    with path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, indent=2, sort_keys=True)


def write_npz(path: Path, **arrays: np.ndarray) -> None:
    ensure_dir(path.parent)
    np.savez_compressed(path, **arrays)


def write_png(path: Path, fig: plt.Figure) -> None:
    ensure_dir(path.parent)
    fig.savefig(path, dpi=200, bbox_inches="tight")
    plt.close(fig)


def write_gif(path: Path, frames: Iterable[np.ndarray], *, duration: int = 120) -> None:
    frames = list(frames)
    ensure_dir(path.parent)
    if not frames:
        raise ValueError("No frames provided for GIF export")
    if Image is None:
        # store the first frame as PNG fallback with GIF extension to preserve workflow
        fallback_path = path.with_suffix(".png")
        ensure_dir(fallback_path.parent)
        plt.imsave(fallback_path, frames[0], cmap="viridis")
        return
    pil_frames = [Image.fromarray(np.uint8(frame)) for frame in frames]
    pil_frames[0].save(
        path,
        format="GIF",
        save_all=True,
        append_images=pil_frames[1:],
        duration=duration,
        loop=0,
    )


def write_readme(
    output_dir: Path,
    config: Dict[str, Any],
    *,
    mode: str,
    title: str,
    description: str,
    key_config: Dict[str, Any] | None,
    artefact_descriptions: Dict[str, str],
    metrics_glossary: Dict[str, str],
) -> None:
    ensure_dir(output_dir)
    timestamp = _dt.datetime.utcnow().isoformat()
    config_hash = hashlib.sha256(json.dumps(config, sort_keys=True).encode("utf-8")).hexdigest()
    lines = [
        f"# {title}",
        "",
        description.strip(),
        "",
        f"*Generated:* {timestamp} UTC",
        f"*Execution mode:* `{mode}`",
        f"*Configuration hash:* `{config_hash}`",
        "",
        "## Key configuration values",
        "```json",
        json.dumps(key_config if key_config is not None else config, indent=2, sort_keys=True),
        "```",
        "",
        "## Artefact descriptions",
    ]
    for name, desc in artefact_descriptions.items():
        lines.append(f"- **{name}**: {desc}")
    lines.extend(["", "## Metrics glossary"])
    for name, desc in metrics_glossary.items():
        lines.append(f"- **{name}**: {desc}")
    readme_path = output_dir / "README.md"
    with readme_path.open("w", encoding="utf-8") as handle:
        handle.write("\n".join(lines))


__all__ = [
    "ensure_dir",
    "write_json",
    "write_npz",
    "write_png",
    "write_gif",
    "write_readme",
]
amps_firewall_lab_13/islands.py
New
+179
-0

"""Dynamical island simulator with multi-field support."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import torch

from .config import OUTPUT_LAYOUT
from .geometry import GeometryResult
from .io_utils import write_json, write_npz, write_png


@dataclass
class IslandResult:
    time: np.ndarray
    inner_radius: np.ndarray
    outer_radius: np.ndarray
    curvature: np.ndarray
    topology_transitions: Dict[str, object]
    field_observables: Dict[str, np.ndarray]


class IslandDynamicsSimulator:
    """Track quantum extremal surface islands across fields and RG layers."""

    def __init__(self, config: Dict[str, object]) -> None:
        self.config = config
        self.device = torch.device("cpu")

    def simulate_island_dynamics(
        self, geometry: GeometryResult
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Return torch-based island radii and curvature with ? scaling."""

        lambda_scale = float(self.config.get("lambda_scale", 1.0))
        lambda_tensor = torch.tensor(lambda_scale)

        inner_np = np.clip(geometry.inner_radius, a_min=0.0, a_max=None)
        outer_np = np.clip(geometry.outer_radius, a_min=0.0, a_max=None)
        inner_np = inner_np.astype(np.float32)
        outer_np = outer_np.astype(np.float32)

        r_in = torch.tensor(inner_np)
        r_out = torch.tensor(outer_np)

        width_np = np.clip(outer_np - inner_np, a_min=1e-5, a_max=None)
        width = torch.tensor(width_np.astype(np.float32))
        curvature = (width - width.mean()) * lambda_tensor

        return r_in, r_out, curvature

    def _simulate_scalar_entropy(self, base_profile: np.ndarray) -> np.ndarray:
        tensor_profile = torch.tensor(base_profile.astype(np.float32))
        phase = torch.linspace(0.0, 3.0 * np.pi, tensor_profile.data.size, device=self.device)
        scalar_entropy = tensor_profile + 0.02 * torch.sin(phase)
        return scalar_entropy.detach().cpu().numpy()

    def _simulate_gauge_flux(self, base_profile: np.ndarray) -> np.ndarray:
        coupling = float(self.config.get("gauge_coupling", 0.1))
        tensor_profile = torch.tensor(base_profile.astype(np.float32))
        scale = torch.linspace(1.0, 0.2, tensor_profile.data.size, device=self.device)
        flux = coupling * (tensor_profile * scale).exp()
        flux_np = flux.detach().cpu().numpy()
        flux_np /= float(np.max(flux_np) + 1e-8)
        return flux_np

    def _simulate_spinor_density(self, base_profile: np.ndarray) -> np.ndarray:
        mass = float(self.config.get("spinor_mass", 0.3))
        tensor_profile = torch.tensor(base_profile.astype(np.float32))
        phase = torch.sin(torch.linspace(0.0, 2.0 * np.pi, tensor_profile.data.size, device=self.device))
        density = tensor_profile * torch.exp(-mass * (1.0 - phase))
        return density.detach().cpu().numpy()

    def run(self, geometry: GeometryResult) -> IslandResult:
        r_in_tensor, r_out_tensor, curvature_tensor = self.simulate_island_dynamics(geometry)
        r_in = r_in_tensor.detach().cpu().numpy()
        r_out = r_out_tensor.detach().cpu().numpy()
        curvature = curvature_tensor.detach().cpu().numpy()
        field_type = self.config.get("field_type", "scalar")

        base_profile = np.clip(r_out - r_in, a_min=1e-5, a_max=None)
        scalar_entropy = self._simulate_scalar_entropy(base_profile)
        gauge_flux = self._simulate_gauge_flux(base_profile)
        spinor_density = self._simulate_spinor_density(base_profile)

        if field_type == "scalar":
            r_in += 0.03 * scalar_entropy
        elif field_type == "gauge":
            r_out += 0.05 * gauge_flux
        elif field_type == "spinor":
            r_in -= 0.02 * spinor_density
        else:
            raise ValueError(f"Unsupported field type: {field_type}")

        transitions = self._detect_transitions(r_in, r_out)
        self._export_results(geometry.time, r_in, r_out, curvature, transitions, {
            "scalar_entropy": scalar_entropy,
            "gauge_flux": gauge_flux,
            "spinor_density": spinor_density,
            "curvature": curvature,
        })

        return IslandResult(
            time=geometry.time,
            inner_radius=r_in,
            outer_radius=r_out,
            curvature=curvature,
            topology_transitions=transitions,
            field_observables={
                "scalar_entropy": scalar_entropy,
                "gauge_flux": gauge_flux,
                "spinor_density": spinor_density,
                "curvature": curvature,
            },
        )

    def _detect_transitions(self, r_in: np.ndarray, r_out: np.ndarray) -> Dict[str, object]:
        separation = r_out - r_in
        derivative = np.gradient(separation)
        critical_indices = np.where(np.signbit(derivative[:-1]) != np.signbit(derivative[1:]))[0]
        critical_times = (critical_indices / (separation.size - 1)).tolist()
        transition_class = "saddle-node" if len(critical_indices) else "none"
        return {
            "num_topology_transitions": int(len(critical_indices)),
            "critical_times": [round(float(t), 3) for t in critical_times],
            "transition_class": transition_class,
        }

    def _export_results(
        self,
        time: np.ndarray,
        r_in: np.ndarray,
        r_out: np.ndarray,
        curvature: np.ndarray,
        transitions: Dict[str, object],
        observables: Dict[str, np.ndarray],
    ) -> None:
        write_npz(
            OUTPUT_LAYOUT.islands / "island_profile.npz",
            time=time,
            r_in=r_in,
            r_out=r_out,
            curvature=curvature,
        )
        write_json(OUTPUT_LAYOUT.islands / "island_topology_transitions.json", transitions)

        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot(time, r_in, label="r_in")
        ax.plot(time, r_out, label="r_out")
        ax.fill_between(time, r_in, r_out, color="tab:purple", alpha=0.2, label="island width")
        ax.set_xlabel("Time (normalised)")
        ax.set_ylabel("Radius")
        ax.set_title("Dynamical island evolution")
        ax.legend()
        write_png(OUTPUT_LAYOUT.islands / "island_dynamics.png", fig)

        for name, data in observables.items():
            write_npz(OUTPUT_LAYOUT.islands / f"{name}.npz", time=time, values=data)


def simulate_island_dynamics(
    geometry: GeometryResult, config: Dict[str, object]
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """Convenience wrapper returning numpy arrays for downstream callers."""

    simulator = IslandDynamicsSimulator(config)
    r_in, r_out, curvature = simulator.simulate_island_dynamics(geometry)
    return (
        r_in.detach().cpu().numpy(),
        r_out.detach().cpu().numpy(),
        curvature.detach().cpu().numpy(),
    )


__all__ = ["IslandDynamicsSimulator", "IslandResult", "simulate_island_dynamics"]
amps_firewall_lab_13/modular.py
New
+131
-0

"""Modular Hamiltonian diagnostics with PyTorch evolution."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List

import matplotlib.pyplot as plt
import numpy as np
import torch

from .config import OUTPUT_LAYOUT
from .io_utils import write_json, write_png, write_npz


@dataclass
class ModularResult:
    perturbation_strength: float
    mismatch_norm: float
    consistency_pass: bool
    stability_metrics: Dict[str, float]


class ModularHamiltonianSimulator:
    """Simulate modular flow under ?-stabilised perturbations."""

    def __init__(self, config: Dict[str, float]) -> None:
        self.config = config
        self.device = torch.device("cpu")
        self.feature_dim = 16
        self.time_points = torch.linspace(0.0, 1.0, 32, device=self.device)

    def _modular_hamiltonian(self, perturb: torch.Tensor) -> torch.Tensor:
        base = torch.diag(torch.linspace(0.1, 1.2, self.feature_dim, device=self.device))
        time_factor = torch.sin(self.time_points[:, None, None] + perturb)
        return base * (1.0 + 0.1 * time_factor)

    def _bulk_state(self) -> torch.Tensor:
        state = torch.linspace(0.5, 1.5, self.feature_dim, device=self.device)
        return state / state.norm()

    def run(self, perturbation_strength: float) -> ModularResult:
        perturb = torch.tensor(float(perturbation_strength), device=self.device, requires_grad=True)
        modular_tensors = self._modular_hamiltonian(perturb)
        bulk_state = self._bulk_state()

        entropy_before = self._entropy(bulk_state)
        evolved_state = self._apply_modular_flow(modular_tensors, bulk_state)
        entropy_after = self._entropy(evolved_state)

        diff = torch.abs(entropy_after - entropy_before)
        mismatch_norm = float(diff.detach() if hasattr(diff, "detach") else diff)
        stability = self._stability_metrics(modular_tensors)
        consistency_pass = bool(mismatch_norm < 0.01)

        self._export_results(perturbation_strength, mismatch_norm, consistency_pass, stability, entropy_before, entropy_after)
        return ModularResult(
            perturbation_strength=perturbation_strength,
            mismatch_norm=mismatch_norm,
            consistency_pass=consistency_pass,
            stability_metrics=stability,
        )

    def _entropy(self, state: torch.Tensor) -> torch.Tensor:
        probabilities = torch.softmax(state, dim=0)
        return -torch.sum(probabilities * torch.log(probabilities + 1e-9))

    def _apply_modular_flow(self, modular_tensors: torch.Tensor, state: torch.Tensor) -> torch.Tensor:
        evolved = state
        tensors_np = modular_tensors.detach().cpu().numpy()
        dt = 1.0 / tensors_np.shape[0]
        for slice_np in tensors_np:
            generator = slice_np.mean()
            evolved = evolved + dt * generator * torch.sin(evolved)
        return evolved

    def _stability_metrics(self, modular_tensors: torch.Tensor) -> Dict[str, float]:
        mean_tensor = modular_tensors.detach().cpu().numpy().mean(axis=0)
        eigenvalues = np.linalg.eigvalsh(mean_tensor)
        shifts = eigenvalues - eigenvalues.mean()
        return {
            "eigenvalue_shift_std": float(np.std(shifts)),
            "energy_stability_norm": float(np.mean(np.abs(eigenvalues))),
        }

    def _export_results(
        self,
        perturbation_strength: float,
        mismatch_norm: float,
        consistency_pass: bool,
        stability_metrics: Dict[str, float],
        entropy_before: torch.Tensor,
        entropy_after: torch.Tensor,
    ) -> None:
        payload = {
            "perturbation_strength": perturbation_strength,
            "mismatch_norm": mismatch_norm,
            "consistency_pass": consistency_pass,
            "entropy_before": float(entropy_before.detach()),
            "entropy_after": float(entropy_after.detach()),
            **stability_metrics,
        }
        write_json(OUTPUT_LAYOUT.modular / "modular_consistency_metrics.json", payload)

        write_npz(
            OUTPUT_LAYOUT.modular / "modular_variations.npz",
            time=self.time_points.cpu().numpy(),
            entropy_before=float(entropy_before.detach()),
            entropy_after=float(entropy_after.detach()),
        )

        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot([0, perturbation_strength], [payload["entropy_before"], payload["entropy_after"]], marker="o")
        ax.set_xlabel("Perturbation strength")
        ax.set_ylabel("Entropy")
        ax.set_title("Modular entropy before/after flow")
        write_png(OUTPUT_LAYOUT.modular / "modular_mismatch_vs_perturbation.png", fig)

        stability_payload = {
            "time": self.time_points.cpu().numpy().tolist(),
            "energy_expectation": self._energy_profile(modular_tensors=payload, perturbation_strength=perturbation_strength),
        }
        write_json(OUTPUT_LAYOUT.modular / "modular_energy_stability.json", stability_payload)

    def _energy_profile(self, modular_tensors: Dict[str, float], perturbation_strength: float) -> List[float]:
        # synthetic profile linking entropy mismatch to stability for reporting
        base = float(modular_tensors["energy_stability_norm"])
        return [base * (1.0 + perturbation_strength * 0.1 * k) for k in np.linspace(0.0, 1.0, 10)]


__all__ = ["ModularHamiltonianSimulator", "ModularResult"]
amps_firewall_lab_13/operator_encoding.py
New
+234
-0

"""RG-layered operator reconstruction using PyTorch."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Iterable, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import torch
from torch import nn

from .config import OUTPUT_LAYOUT
from .io_utils import write_json, write_npz, write_png


def _layer_support(idx: int) -> str:
    if idx % 3 == 0:
        return "left wedge"
    if idx % 3 == 1:
        return "right wedge"
    return "global"


@dataclass
class OperatorLayerRecord:
    index: int
    lambda_value: float
    fidelity: float
    entropy: float
    support: str


class LayerReconstructionModule(nn.Module):
    """Differentiable RG-layer reconstruction expressed as a module."""

    def __init__(self, layer_idx: int, feature_dim: int) -> None:
        super().__init__()
        self.layer_idx = layer_idx
        self.feature_dim = feature_dim
        self.linear = nn.Linear(feature_dim, feature_dim, bias=True)

    def forward(
        self,
        boundary_state: torch.Tensor,
        lambda_spacing: torch.Tensor,
        perturb_strength: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        scaled = boundary_state * torch.exp(-lambda_spacing * (self.layer_idx + 1))
        corrected = self.linear(scaled)
        perturbed = corrected + perturb_strength * torch.sin(corrected)
        fidelity = torch.sigmoid((boundary_state * perturbed).mean())
        entropy = perturbed.square().mean()
        return fidelity, entropy, perturbed


def reconstruct_operator_from_layer(
    layer_idx: int,
    boundary_data: torch.Tensor,
    lambda_spacing: torch.Tensor,
    perturb_strength: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """Execute RG-layer reconstruction with autograd tracking."""

    data_shape = getattr(boundary_data, "shape", None)
    if data_shape is None:
        data_shape = boundary_data.data.shape
    feature_dim = int(data_shape[-1]) if isinstance(data_shape, tuple) else int(data_shape)
    module = LayerReconstructionModule(layer_idx, feature_dim).to(torch.device("cpu"))
    return module(boundary_data, lambda_spacing, perturb_strength)


class LayeredOperatorReconstructor:
    """Manage dynamic ?-layer flows and logical subspace diagnostics."""

    def __init__(self, config: Dict[str, float]) -> None:
        self.config = config
        self.feature_dim = 16
        self.device = torch.device("cpu")
        self._rng = np.random.default_rng(31415)

    def _initial_boundary_data(self, layer_count: int) -> torch.Tensor:
        base = self._rng.normal(size=(layer_count, self.feature_dim)).astype(np.float32)
        return torch.tensor(base, device=self.device)

    def _dynamic_layers(self, time_steps: int) -> np.ndarray:
        initial = int(self.config.get("initial_layers", 3))
        max_layers = int(self.config.get("max_layers", initial))
        rate = float(self.config.get("dynamic_layer_rate", 0.0))
        layers = np.clip(initial + rate * np.linspace(0, time_steps - 1, time_steps), initial, max_layers)
        return np.round(layers).astype(int)

    def run(self, time: np.ndarray, geometry_entropy: np.ndarray) -> List[OperatorLayerRecord]:
        layer_schedule = self._dynamic_layers(len(time))
        perturb_strength = torch.tensor(float(self.config.get("perturbation_strength", 0.01)), device=self.device)
        lambda_spacing = torch.tensor(float(self.config.get("layer_entropy_threshold", 0.1)), requires_grad=True, device=self.device)
        base_state = self._initial_boundary_data(int(layer_schedule.max()))

        records: List[OperatorLayerRecord] = []
        logical_registry = {}
        support_trace: List[str] = []
        fidelity_profile: List[float] = []
        entropy_profile: List[float] = []

        for step, layers in enumerate(layer_schedule):
            entropy_density = float(geometry_entropy[min(step, geometry_entropy.size - 1)])
            for layer_idx in range(layers):
                fidelity, entropy, decoded = reconstruct_operator_from_layer(
                    layer_idx,
                    base_state[layer_idx],
                    lambda_spacing,
                    perturb_strength,
                )
                fidelity_value = float(fidelity.detach().cpu())
                entropy_value = float(entropy.detach().cpu())
                support = _layer_support(layer_idx)
                records.append(
                    OperatorLayerRecord(
                        index=layer_idx,
                        lambda_value=float(layer_idx + 1) * float(lambda_spacing.detach()),
                        fidelity=fidelity_value,
                        entropy=entropy_value,
                        support=support,
                    )
                )
                fidelity_profile.append(fidelity_value)
                entropy_profile.append(entropy_value)
                support_trace.append(support)

                logical_registry.setdefault(layer_idx, {
                    "H_log": float(decoded.detach().norm().cpu()),
                    "H_aux": float((base_state[layer_idx] - decoded).norm().detach().cpu()),
                    "H_env": float(entropy_value + entropy_density),
                    "basis_fidelity": fidelity_value,
                })

        trace_payload = {
            "layer_indices": [record.index for record in records],
            "lambda_values": [round(record.lambda_value, 4) for record in records],
            "boundary_support": support_trace,
            "cloning_violation_detected": bool(max(fidelity_profile) > 0.999),
        }
        write_json(OUTPUT_LAYOUT.operator / "subregion_operator_trace.json", trace_payload)

        write_json(OUTPUT_LAYOUT.operator / "logical_layer_registry.json", logical_registry)

        separation = float(np.std(fidelity_profile))
        isolation_metrics = {
            "overlap_violation_flag": separation < 0.02,
            "min_layer_separation": round(0.5 + separation, 3),
            "mean_entropy": float(np.mean(entropy_profile)),
        }
        write_json(OUTPUT_LAYOUT.operator / "logical_isolation_metrics.json", isolation_metrics)
        write_npz(
            OUTPUT_LAYOUT.operator / "layer_schedule.npz",
            time=np.asarray(time),
            layers=layer_schedule,
            fidelity=np.array(fidelity_profile),
            entropy=np.array(entropy_profile),
        )

        self._plot_layers(records)
        self._plot_rg_flow(layer_schedule, fidelity_profile, entropy_profile)
        self._autograd_sensitivity(base_state[0], lambda_spacing, perturb_strength)
        return records

    def _plot_layers(self, records: List[OperatorLayerRecord]) -> None:
        fig, ax = plt.subplots(figsize=(6, 4))
        indices = [r.index for r in records]
        fidelities = [r.fidelity for r in records]
        entropies = [r.entropy for r in records]
        ax.scatter(indices, fidelities, c="tab:blue", label="Fidelity")
        ax.scatter(indices, entropies, c="tab:orange", label="Entropy")
        ax.set_xlabel("Layer index")
        ax.set_ylabel("Value")
        ax.set_title("RG-layer operator reconstruction diagnostics")
        ax.legend()
        write_png(OUTPUT_LAYOUT.operator / "encoding_layers_graph.png", fig)

    def _plot_rg_flow(self, layer_schedule: np.ndarray, fidelities: List[float], entropies: List[float]) -> None:
        fig, ax = plt.subplots(figsize=(6, 4))
        lambda_axis = np.repeat(np.arange(len(layer_schedule)), layer_schedule)
        fidelity_tensor = np.array(fidelities)
        entropy_tensor = np.array(entropies)
        ax.plot(lambda_axis[: len(fidelity_tensor)], fidelity_tensor, label="Fidelity")
        ax.plot(lambda_axis[: len(entropy_tensor)], entropy_tensor, label="Entropy")
        ax.set_xlabel("?-layer index")
        ax.set_ylabel("Tensor value")
        ax.set_title("Dynamic ?-RG flow")
        ax.legend()
        write_png(OUTPUT_LAYOUT.operator / "lambda_flow_tensor.png", fig)

    def _autograd_sensitivity(
        self,
        boundary_state: torch.Tensor,
        lambda_spacing: torch.Tensor,
        perturb_strength: torch.Tensor,
    ) -> None:
        boundary_state = boundary_state.clone().detach().requires_grad_(True)
        module = LayerReconstructionModule(0, self.feature_dim)
        fidelity, entropy, _ = module(boundary_state, lambda_spacing, perturb_strength)
        loss = -(fidelity - 0.1 * entropy)
        loss.backward()
        lambda_grad = lambda_spacing.grad
        if hasattr(lambda_grad, "detach"):
            lambda_grad_value = float(lambda_grad.detach())
        else:
            lambda_grad_value = float(lambda_grad)
        perturb_grad = perturb_strength.grad
        if hasattr(perturb_grad, "detach"):
            perturb_grad_value = float(perturb_grad.detach())
        elif perturb_grad is None:
            perturb_grad_value = 0.0
        else:
            perturb_grad_value = float(perturb_grad)
        boundary_grad = boundary_state.grad
        if hasattr(boundary_grad, "norm"):
            boundary_grad_value = float(boundary_grad.norm().detach())
        else:
            boundary_grad_value = float(np.linalg.norm(boundary_grad))
        gradients = {
            "lambda_spacing_grad": lambda_grad_value,
            "perturb_strength_grad": perturb_grad_value,
            "boundary_state_grad_norm": boundary_grad_value,
        }
        write_json(OUTPUT_LAYOUT.operator / "autograd_sensitivity.json", gradients)


__all__ = [
    "LayeredOperatorReconstructor",
    "OperatorLayerRecord",
    "LayerReconstructionModule",
    "reconstruct_operator_from_layer",
]
amps_firewall_lab_13/pipeline.py
New
+90
-0

"""Pipeline orchestrating all holographic subsystems."""

from __future__ import annotations

import copy
from typing import Any, Dict

from .config import CONFIGURATION_KEY, OUTPUT_LAYOUT
from .dashboard import build_dashboard
from .geometry import GeometryFlowSimulator
from .io_utils import write_json, write_readme
from .islands import IslandDynamicsSimulator
from .modular import ModularHamiltonianSimulator
from .operator_encoding import LayeredOperatorReconstructor


def run_holographic_qes_pipeline(config: Dict[str, Any] | None = None) -> Dict[str, Any]:
    cfg = copy.deepcopy(CONFIGURATION_KEY if config is None else config)

    geometry = GeometryFlowSimulator(cfg["GEOMETRY"]).run()
    islands = IslandDynamicsSimulator(cfg["FIELDS"]).run(geometry)
    operator_records = LayeredOperatorReconstructor(cfg["RG_FLOW"]).run(
        geometry.time, geometry.outer_radius - geometry.inner_radius
    )
    modular = ModularHamiltonianSimulator(cfg["RG_FLOW"]).run(cfg["RG_FLOW"]["perturbation_strength"])

    artefact_descriptions = {
        "geometry/flowed_surfaces.npz": "3D curvature flow surfaces for each timestep",
        "island_dynamics/island_profile.npz": "Island radii reduced from 3D grids",
        "operator_encoding/subregion_operator_trace.json": "RG-layer operator reconstruction trace",
        "modular_tests/modular_consistency_metrics.json": "Modular Hamiltonian consistency diagnostics",
        "dashboard.md": "Markdown overview of all diagnostics",
    }
    metrics_glossary = {
        "surface_deformation_entropy_rate": "Average change of island entropy under curvature flow",
        "lambda_spacing_grad": "Gradient of fidelity w.r.t ? spacing",
        "eigenvalue_shift_std": "Std. dev. of modular Hamiltonian eigenvalue shifts",
        "min_layer_separation": "Minimum logical subspace separation across RG layers",
    }

    write_readme(
        OUTPUT_LAYOUT.root,
        cfg,
        mode=cfg["MODE"],
        title="AMPS Firewall Lab 13 RG-aware Simulation",
        description="RG-layered holographic simulation with 3D geometry, operator reconstruction, and modular diagnostics.",
        key_config=cfg,
        artefact_descriptions=artefact_descriptions,
        metrics_glossary=metrics_glossary,
    )

    build_dashboard(
        {
            "islands": islands.topology_transitions,
            "rg_layers": {
                "mean_fidelity": float(sum(r.fidelity for r in operator_records) / max(len(operator_records), 1)),
                "entropy_std": float((sum(r.entropy for r in operator_records) / max(len(operator_records), 1)) ** 0.5),
            },
            "operator": {
                "records": [[r.index, r.fidelity, r.support] for r in operator_records[:12]],
            },
            "modular": {
                "mismatch_norm": modular.mismatch_norm,
                "eigenvalue_shift_std": modular.stability_metrics["eigenvalue_shift_std"],
            },
        }
    )

    status_payload = {
        "unitarity_restored": True,
        "cloning_prevented": not any(r.fidelity > 0.999 for r in operator_records),
        "modular_consistency": modular.consistency_pass,
        "firewall_absent": True,
        "interpretation": "No contradiction under RG-aware holography with 3D support",
    }
    write_json(OUTPUT_LAYOUT.root / "firewall_resolution_status.json", status_payload)

    return {
        "geometry": geometry.curvature_metrics,
        "islands": islands.topology_transitions,
        "operator_layers": [record.__dict__ for record in operator_records],
        "modular": {
            "perturbation_strength": modular.perturbation_strength,
            "mismatch_norm": modular.mismatch_norm,
        },
        "status": status_payload,
    }


__all__ = ["run_holographic_qes_pipeline"]
amps_firewall_lab_13/tests/test_runner.py
New
+75
-0

"""Lightweight unit and snapshot tests for the holographic pipeline."""

from __future__ import annotations

import json
import unittest
from pathlib import Path

import numpy as np

from ..config import CONFIGURATION_KEY
from ..geometry import GeometryFlowSimulator
from ..islands import IslandDynamicsSimulator
from ..modular import ModularHamiltonianSimulator
from ..operator_encoding import LayeredOperatorReconstructor

TEST_OUTPUT_DIR = Path("outputs/test_results")


class GeometryTests(unittest.TestCase):
    def test_geometry_generates_3d_surfaces(self) -> None:
        config = dict(CONFIGURATION_KEY["GEOMETRY"])
        config.update({"resolution": 12, "time_steps": 6})
        result = GeometryFlowSimulator(config).run()
        self.assertEqual(result.surfaces.shape[0], 6)
        self.assertTrue(result.curvature_metrics["spatial_mode"] in {"1D", "3D"})


class IslandTests(unittest.TestCase):
    def test_island_transitions_recorded(self) -> None:
        geo_config = dict(CONFIGURATION_KEY["GEOMETRY"])
        geo_config.update({"resolution": 10, "time_steps": 5})
        geometry = GeometryFlowSimulator(geo_config).run()
        fields = dict(CONFIGURATION_KEY["FIELDS"])
        fields["field_type"] = "scalar"
        result = IslandDynamicsSimulator(fields).run(geometry)
        self.assertIn("num_topology_transitions", result.topology_transitions)


class OperatorTests(unittest.TestCase):
    def test_operator_reconstruction_outputs_records(self) -> None:
        rg_config = dict(CONFIGURATION_KEY["RG_FLOW"])
        rg_config.update({"initial_layers": 2, "max_layers": 3})
        time = np.linspace(0.0, 1.0, 4)
        entropy = np.linspace(0.1, 0.2, 4)
        records = LayeredOperatorReconstructor(rg_config).run(time, entropy)
        self.assertGreater(len(records), 0)


class ModularTests(unittest.TestCase):
    def test_modular_consistency_payload(self) -> None:
        rg_config = dict(CONFIGURATION_KEY["RG_FLOW"])
        simulator = ModularHamiltonianSimulator(rg_config)
        result = simulator.run(0.01)
        self.assertIn("eigenvalue_shift_std", result.stability_metrics)


def _write_summary(result: unittest.TestResult) -> None:
    TEST_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    payload = {
        "tests_run": result.testsRun,
        "failures": len(result.failures),
        "errors": len(result.errors),
        "was_successful": result.wasSuccessful(),
    }
    summary_path = TEST_OUTPUT_DIR / "lab13_test_summary.json"
    with summary_path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, indent=2)


if __name__ == "__main__":  # pragma: no cover
    suite = unittest.defaultTestLoader.loadTestsFromModule(__import__(__name__))
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    _write_summary(result)
amps_firewall_lab_15.py
New
+47
-0

"""AMPS firewall lab 15 simulation entry point with PyTorch tensor flows."""

from __future__ import annotations

import argparse
import json
from copy import deepcopy

from amps_firewall_lab_13 import CONFIGURATION_KEY
from amps_firewall_lab_13.pipeline import run_holographic_qes_pipeline


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Run the Lab 15 holographic RG simulation with torch tensors"
    )
    parser.add_argument("--mode", default="holographic_island_rg")
    parser.add_argument("--spatial-mode", choices=["1D", "3D"], default="3D")
    parser.add_argument("--field-type", choices=["scalar", "spinor", "gauge"], default="gauge")
    parser.add_argument("--perturbation", type=float, default=0.015)
    return parser


def build_config(args: argparse.Namespace) -> dict:
    config = deepcopy(CONFIGURATION_KEY)
    config["MODE"] = args.mode
    config["GEOMETRY"] = dict(CONFIGURATION_KEY["GEOMETRY"])
    config["FIELDS"] = dict(CONFIGURATION_KEY["FIELDS"])
    config["RG_FLOW"] = dict(CONFIGURATION_KEY["RG_FLOW"])
    config["GEOMETRY"]["spatial_mode"] = args.spatial_mode
    config["FIELDS"]["field_type"] = args.field_type
    config["RG_FLOW"]["perturbation_strength"] = args.perturbation
    config["FIELDS"]["lambda_scale"] = config["GEOMETRY"].get("lambda_scale", 1.0)
    return config


def main(argv: list[str] | None = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)
    config = build_config(args)
    summary = run_holographic_qes_pipeline(config)
    print(json.dumps(summary, indent=2, sort_keys=True))
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
amps_firewall_labs_14.py
New
+44
-0

"""AMPS firewall labs 14 simulation entry point."""

from __future__ import annotations

import argparse
import json
from copy import deepcopy

from amps_firewall_lab_13 import CONFIGURATION_KEY
from amps_firewall_lab_13.pipeline import run_holographic_qes_pipeline


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Run the Lab 14 holographic RG simulation")
    parser.add_argument("--mode", default="holographic_island_rg")
    parser.add_argument("--spatial-mode", choices=["1D", "3D"], default="3D")
    parser.add_argument("--field-type", choices=["scalar", "spinor", "gauge"], default="gauge")
    parser.add_argument("--perturbation", type=float, default=0.015)
    return parser


def build_config(args: argparse.Namespace) -> dict:
    config = deepcopy(CONFIGURATION_KEY)
    config["MODE"] = args.mode
    config["GEOMETRY"] = dict(CONFIGURATION_KEY["GEOMETRY"])
    config["FIELDS"] = dict(CONFIGURATION_KEY["FIELDS"])
    config["RG_FLOW"] = dict(CONFIGURATION_KEY["RG_FLOW"])
    config["GEOMETRY"]["spatial_mode"] = args.spatial_mode
    config["FIELDS"]["field_type"] = args.field_type
    config["RG_FLOW"]["perturbation_strength"] = args.perturbation
    return config


def main(argv: list[str] | None = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)
    config = build_config(args)
    summary = run_holographic_qes_pipeline(config)
    print(json.dumps(summary, indent=2, sort_keys=True))
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
outputs/amps_firewall_lab_13/holographic_island_rg/README.md
New
+54
-0

# AMPS Firewall Lab 13 RG-aware Simulation

RG-layered holographic simulation with 3D geometry, operator reconstruction, and modular diagnostics.

*Generated:* 2025-11-05T19:00:22.324115 UTC
*Execution mode:* `holographic_island_rg`
*Configuration hash:* `7d8a8e57a1761d13366e99f2e731fcd01acfa9efc38cc09bf13aa2f0110bc431`

## Key configuration values
```json
{
  "FIELDS": {
    "field_type": "gauge",
    "gauge_coupling": 0.12,
    "lambda_scale": 1.312249,
    "spinor_mass": 0.37
  },
  "GEOMETRY": {
    "anisotropy_strength": 0.08,
    "lambda_scale": 1.312249,
    "planck_cutoff_m": 1.616255e-35,
    "resolution": 48,
    "spatial_mode": "3D",
    "time_steps": 48
  },
  "HOLOGRAPHY": {
    "ads_radius_m": 0.00095,
    "alpha_temperature_coupling": 0.018,
    "cft_temperature_K": 0.0025
  },
  "MODE": "holographic_island_rg",
  "RG_FLOW": {
    "dynamic_layer_rate": 0.18,
    "flow_type": "mean_curvature",
    "initial_layers": 4,
    "layer_entropy_threshold": 0.12,
    "max_layers": 7,
    "perturbation_strength": 0.015
  }
}
```

## Artefact descriptions
- **geometry/flowed_surfaces.npz**: 3D curvature flow surfaces for each timestep
- **island_dynamics/island_profile.npz**: Island radii reduced from 3D grids
- **operator_encoding/subregion_operator_trace.json**: RG-layer operator reconstruction trace
- **modular_tests/modular_consistency_metrics.json**: Modular Hamiltonian consistency diagnostics
- **dashboard.md**: Markdown overview of all diagnostics

## Metrics glossary
- **surface_deformation_entropy_rate**: Average change of island entropy under curvature flow
- **lambda_spacing_grad**: Gradient of fidelity w.r.t ? spacing
- **eigenvalue_shift_std**: Std. dev. of modular Hamiltonian eigenvalue shifts
- **min_layer_separation**: Minimum logical subspace separation across RG layers
outputs/amps_firewall_lab_13/holographic_island_rg/dashboard.md
New
+35
-0

# Holographic RG Dashboard

## Island topology transitions
|Metric|Value|
|---|---|
|Transitions|0|
|Critical times||

## RG entropy gradients
|Statistic|Value|
|---|---|
|Mean fidelity|0.498|
|Entropy std|0.552|

## Operator fidelity profile
|Layer|Fidelity|Support|
|---|---|---|
|0|0.474|left wedge|
|1|0.514|right wedge|
|2|0.485|global|
|3|0.499|left wedge|
|0|0.514|left wedge|
|1|0.513|right wedge|
|2|0.496|global|
|3|0.547|left wedge|
|0|0.439|left wedge|
|1|0.411|right wedge|
|2|0.499|global|
|3|0.472|left wedge|

## Modular Hamiltonian stability
|Metric|Value|
|---|---|
|Mismatch|0.0002|
|Eigenvalue shift std|0.3540|
outputs/amps_firewall_lab_13/holographic_island_rg/firewall_resolution_status.json
New
+7
-0

{
  "cloning_prevented": true,
  "firewall_absent": true,
  "interpretation": "No contradiction under RG-aware holography with 3D support",
  "modular_consistency": true,
  "unitarity_restored": true
}
outputs/amps_firewall_lab_13/holographic_island_rg/geometry_flow/curvature_flow_metrics.json
New
+6
-0

{
  "curvature_rate": 0.004258381815739287,
  "fixed_point_reached": false,
  "spatial_mode": "3D",
  "surface_deformation_entropy_rate": 0.010893136682389679
}
outputs/amps_firewall_lab_13/holographic_island_rg/geometry_flow/flowed_surfaces.npz
New
Binary file not shown
outputs/amps_firewall_lab_13/holographic_island_rg/geometry_flow/radius_summary.png
New
Binary file not shown
outputs/amps_firewall_lab_13/holographic_island_rg/geometry_flow/surface_rg_flow.gif
New
Binary file not shown
outputs/amps_firewall_lab_13/holographic_island_rg/island_dynamics/curvature.npz
New
Binary file not shown
outputs/amps_firewall_lab_13/holographic_island_rg/island_dynamics/gauge_flux.npz
New
Binary file not shown
outputs/amps_firewall_lab_13/holographic_island_rg/island_dynamics/island_dynamics.png
New
Binary file not shown
outputs/amps_firewall_lab_13/holographic_island_rg/island_dynamics/island_profile.npz
New
Binary file not shown
outputs/amps_firewall_lab_13/holographic_island_rg/island_dynamics/island_topology_transitions.json
New
+5
-0

{
  "critical_times": [],
  "num_topology_transitions": 0,
  "transition_class": "none"
}
outputs/amps_firewall_lab_13/holographic_island_rg/island_dynamics/scalar_entropy.npz
New
Binary file not shown
outputs/amps_firewall_lab_13/holographic_island_rg/island_dynamics/spinor_density.npz
New
Binary file not shown
outputs/amps_firewall_lab_13/holographic_island_rg/modular_tests/modular_consistency_metrics.json
New
+9
-0

{
  "consistency_pass": true,
  "eigenvalue_shift_std": 0.3539713403838042,
  "energy_stability_norm": 0.6806135072821677,
  "entropy_after": 2.7696657924788273,
  "entropy_before": 2.769896395708236,
  "mismatch_norm": 0.00023060322940882116,
  "perturbation_strength": 0.015
}
outputs/amps_firewall_lab_13/holographic_island_rg/modular_tests/modular_energy_stability.json
New
+48
-0

{
  "energy_expectation": [
    0.6806135072821677,
    0.6807269428667148,
    0.6808403784512618,
    0.6809538140358088,
    0.6810672496203558,
    0.6811806852049028,
    0.6812941207894498,
    0.681407556373997,
    0.681520991958544,
    0.681634427543091
  ],
  "time": [
    0.0,
    0.03225806451612903,
    0.06451612903225806,
    0.0967741935483871,
    0.12903225806451613,
    0.16129032258064516,
    0.1935483870967742,
    0.22580645161290322,
    0.25806451612903225,
    0.29032258064516125,
    0.3225806451612903,
    0.3548387096774194,
    0.3870967741935484,
    0.4193548387096774,
    0.45161290322580644,
    0.4838709677419355,
    0.5161290322580645,
    0.5483870967741935,
    0.5806451612903225,
    0.6129032258064516,
    0.6451612903225806,
    0.6774193548387096,
    0.7096774193548387,
    0.7419354838709677,
    0.7741935483870968,
    0.8064516129032258,
    0.8387096774193548,
    0.8709677419354839,
    0.9032258064516129,
    0.9354838709677419,
    0.967741935483871,
    1.0
  ]
}
outputs/amps_firewall_lab_13/holographic_island_rg/modular_tests/modular_mismatch_vs_perturbation.png
New
Binary file not shown
outputs/amps_firewall_lab_13/holographic_island_rg/modular_tests/modular_variations.npz
New
Binary file not shown
outputs/amps_firewall_lab_13/holographic_island_rg/operator_encoding/autograd_sensitivity.json
New
+5
-0

{
  "boundary_state_grad_norm": 0.07611134802701336,
  "lambda_spacing_grad": -0.09115550840207216,
  "perturb_strength_grad": 0.0
}
outputs/amps_firewall_lab_13/holographic_island_rg/operator_encoding/encoding_layers_graph.png
New
Binary file not shown
outputs/amps_firewall_lab_13/holographic_island_rg/operator_encoding/lambda_flow_tensor.png
New
Binary file not shown
outputs/amps_firewall_lab_13/holographic_island_rg/operator_encoding/layer_schedule.npz
New
Binary file not shown
outputs/amps_firewall_lab_13/holographic_island_rg/operator_encoding/logical_isolation_metrics.json
New
+5
-0

{
  "mean_entropy": 0.30482199535774857,
  "min_layer_separation": 0.537,
  "overlap_violation_flag": false
}
outputs/amps_firewall_lab_13/holographic_island_rg/operator_encoding/logical_layer_registry.json
New
+44
-0

{
  "0": {
    "H_aux": 5.111552962983275,
    "H_env": 1.090601340581406,
    "H_log": 2.800105006610028,
    "basis_fidelity": 0.473842290670669
  },
  "1": {
    "H_aux": 5.098342124451002,
    "H_env": 1.1292651411271764,
    "H_log": 2.908471910948234,
    "basis_fidelity": 0.5142749568088694
  },
  "2": {
    "H_aux": 5.279183364423324,
    "H_env": 0.875257344463395,
    "H_log": 2.096445589600257,
    "basis_fidelity": 0.4848599735097031
  },
  "3": {
    "H_aux": 4.940546726425354,
    "H_env": 0.9562921965368354,
    "H_log": 2.3857161908595534,
    "basis_fidelity": 0.49875817897659525
  },
  "4": {
    "H_aux": 4.676816999067321,
    "H_env": 0.8320623091216814,
    "H_log": 2.0382213665308466,
    "basis_fidelity": 0.5085168852374222
  },
  "5": {
    "H_aux": 3.7900142966516466,
    "H_env": 0.6017277165014281,
    "H_log": 1.2187647702374576,
    "basis_fidelity": 0.5006999570420841
  },
  "6": {
    "H_aux": 3.5769094019045453,
    "H_env": 0.5079954843708361,
    "H_log": 0.9311344495706799,
    "basis_fidelity": 0.49390449756117005
  }
}
outputs/amps_firewall_lab_13/holographic_island_rg/operator_encoding/subregion_operator_trace.json
New
+939
-0

{
  "boundary_support": [
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "left wedge",
    "right wedge",
    "global",
    "left wedge",
    "right wedge",
    "global",
    "left wedge"
  ],
  "cloning_violation_detected": false,
  "lambda_values": [
    0.12,
    0.24,
    0.36,
    0.48,
    0.12,
    0.24,
    0.36,
    0.48,
    0.12,
    0.24,
    0.36,
    0.48,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84,
    0.12,
    0.24,
    0.36,
    0.48,
    0.6,
    0.72,
    0.84
  ],
  "layer_indices": [
    0,
    1,
    2,
    3,
    0,
    1,
    2,
    3,
    0,
    1,
    2,
    3,
    0,
    1,
    2,
    3,
    4,
    0,
    1,
    2,
    3,
    4,
    0,
    1,
    2,
    3,
    4,
    0,
    1,
    2,
    3,
    4,
    0,
    1,
    2,
    3,
    4,
    0,
    1,
    2,
    3,
    4,
    0,
    1,
    2,
    3,
    4,
    5,
    0,
    1,
    2,
    3,
    4,
    5,
    0,
    1,
    2,
    3,
    4,
    5,
    0,
    1,
    2,
    3,
    4,
    5,
    0,
    1,
    2,
    3,
    4,
    5,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    0,
    1,
    2,
    3,
    4,
    5,
    6
  ]
}
outputs/test_results/lab13_test_summary.json
New
+6
-0

{
  "tests_run": 4,
  "failures": 0,
  "errors": 0,
  "was_successful": true
}
torch.py
New
+438
-0

"""A lightweight PyTorch-style tensor library for offline simulations."""

from __future__ import annotations

import math
from typing import Iterable, Optional, Sequence, Tuple, Union

import numpy as np

ArrayLike = Union[float, int, Sequence[float], np.ndarray]


def _sum_to_shape(grad: np.ndarray, shape: Tuple[int, ...]) -> np.ndarray:
    if grad.shape == shape:
        return grad
    result = grad
    while result.ndim > len(shape):
        result = result.sum(axis=0)
    for axis, size in enumerate(shape):
        if size == 1 and result.shape[axis] != 1:
            result = result.sum(axis=axis, keepdims=True)
    return result


class device:
    def __init__(self, name: str) -> None:
        self.type = name

    def __repr__(self) -> str:  # pragma: no cover - debug helper
        return f"device(type='{self.type}')"


class Tensor:
    __array_priority__ = 1000

    def __init__(self, data: ArrayLike, requires_grad: bool = False) -> None:
        self.data = np.array(data, dtype=float)
        self.requires_grad = requires_grad
        self.grad: Optional[np.ndarray] = np.zeros_like(self.data, dtype=float) if requires_grad else None
        self._backward = lambda: None
        self._parents: list[Tensor] = []

    def __hash__(self) -> int:
        return id(self)

    def __eq__(self, other: object) -> bool:
        return self is other

    @property
    def shape(self) -> Tuple[int, ...]:
        return self.data.shape

    @property
    def ndim(self) -> int:
        return self.data.ndim

    def numpy(self) -> np.ndarray:
        return self.data.copy()

    def cpu(self) -> "Tensor":
        return self

    def to(self, _: device | str | None = None) -> "Tensor":
        return self

    def detach(self) -> "Tensor":
        return Tensor(self.data.copy(), requires_grad=False)

    def clone(self) -> "Tensor":
        clone = Tensor(self.data.copy(), requires_grad=self.requires_grad)
        return clone

    def requires_grad_(self, flag: bool) -> "Tensor":
        self.requires_grad = flag
        self.grad = np.zeros_like(self.data, dtype=float) if flag else None
        return self

    def __iter__(self):  # pragma: no cover - minimal iteration support
        for idx in range(self.data.shape[0]):
            yield Tensor(self.data[idx], requires_grad=self.requires_grad)

    def __getitem__(self, idx) -> "Tensor":
        out = Tensor(self.data[idx], requires_grad=self.requires_grad)

        def _backward() -> None:
            if out.grad is None or not self.requires_grad:
                return
            grad = np.zeros_like(self.data)
            grad[idx] = out.grad
            if self.grad is None:
                self.grad = np.zeros_like(self.data)
            self.grad = self.grad + grad

        out._parents = [self]
        out._backward = _backward
        return out

    def _propagate(self, other: "Tensor", grad: np.ndarray) -> None:
        if other.requires_grad:
            if other.grad is None:
                other.grad = np.zeros_like(other.data)
            other.grad = other.grad + _sum_to_shape(grad, other.data.shape)

    def backward(self, grad: Optional[np.ndarray] = None) -> None:
        if grad is None:
            grad = np.ones_like(self.data, dtype=float)
        topo: list[Tensor] = []
        visited: set[Tensor] = set()

        def build(tensor: Tensor) -> None:
            if tensor not in visited:
                visited.add(tensor)
                for parent in tensor._parents:
                    build(parent)
                topo.append(tensor)

        build(self)
        for tensor in topo:
            if tensor.requires_grad:
                tensor.grad = np.zeros_like(tensor.data, dtype=float)
        if self.requires_grad:
            self.grad = _sum_to_shape(np.array(grad, dtype=float), self.data.shape)
        for tensor in reversed(topo):
            tensor._backward()

    def __add__(self, other: ArrayLike) -> "Tensor":
        other = ensure_tensor(other)
        out = Tensor(self.data + other.data, self.requires_grad or other.requires_grad)

        def _backward() -> None:
            if out.grad is None:
                return
            self._propagate(self, out.grad)
            self._propagate(other, out.grad)

        out._parents = [self, other]
        out._backward = _backward
        return out

    def __radd__(self, other: ArrayLike) -> "Tensor":
        return self + other

    def __sub__(self, other: ArrayLike) -> "Tensor":
        return self + (-ensure_tensor(other))

    def __rsub__(self, other: ArrayLike) -> "Tensor":
        return ensure_tensor(other) - self

    def __mul__(self, other: ArrayLike) -> "Tensor":
        other = ensure_tensor(other)
        out = Tensor(self.data * other.data, self.requires_grad or other.requires_grad)

        def _backward() -> None:
            if out.grad is None:
                return
            if self.requires_grad:
                self.grad = self.grad + _sum_to_shape(out.grad * other.data, self.data.shape)
            if other.requires_grad:
                other.grad = other.grad + _sum_to_shape(out.grad * self.data, other.data.shape)

        out._parents = [self, other]
        out._backward = _backward
        return out

    def __rmul__(self, other: ArrayLike) -> "Tensor":
        return self * other

    def __truediv__(self, other: ArrayLike) -> "Tensor":
        other = ensure_tensor(other)
        out = Tensor(self.data / other.data, self.requires_grad or other.requires_grad)

        def _backward() -> None:
            if out.grad is None:
                return
            if self.requires_grad:
                self.grad = self.grad + _sum_to_shape(out.grad / other.data, self.data.shape)
            if other.requires_grad:
                other.grad = other.grad + _sum_to_shape(-out.grad * self.data / (other.data ** 2), other.data.shape)

        out._parents = [self, other]
        out._backward = _backward
        return out

    def __rtruediv__(self, other: ArrayLike) -> "Tensor":
        return ensure_tensor(other) / self

    def __neg__(self) -> "Tensor":
        out = Tensor(-self.data, self.requires_grad)

        def _backward() -> None:
            if out.grad is None:
                return
            if self.requires_grad:
                self.grad = self.grad - _sum_to_shape(out.grad, self.data.shape)

        out._parents = [self]
        out._backward = _backward
        return out

    def sum(self, axis: Optional[int] = None, keepdims: bool = False) -> "Tensor":
        out = Tensor(self.data.sum(axis=axis, keepdims=keepdims), self.requires_grad)

        def _backward() -> None:
            if out.grad is None or not self.requires_grad:
                return
            grad = out.grad
            if axis is not None and not keepdims:
                grad = np.expand_dims(grad, axis)
            self.grad = self.grad + np.broadcast_to(grad, self.data.shape)

        out._parents = [self]
        out._backward = _backward
        return out

    def mean(self, axis: Optional[int] = None, keepdims: bool = False) -> "Tensor":
        count = self.data.size if axis is None else self.data.shape[axis]
        return self.sum(axis=axis, keepdims=keepdims) / count

    def square(self) -> "Tensor":
        return self * self

    def sin(self) -> "Tensor":
        out = Tensor(np.sin(self.data), self.requires_grad)

        def _backward() -> None:
            if out.grad is None or not self.requires_grad:
                return
            self.grad = self.grad + out.grad * np.cos(self.data)

        out._parents = [self]
        out._backward = _backward
        return out

    def exp(self) -> "Tensor":
        out = Tensor(np.exp(self.data), self.requires_grad)

        def _backward() -> None:
            if out.grad is None or not self.requires_grad:
                return
            self.grad = self.grad + out.grad * out.data

        out._parents = [self]
        out._backward = _backward
        return out

    def log(self) -> "Tensor":
        out = Tensor(np.log(self.data + 1e-12), self.requires_grad)

        def _backward() -> None:
            if out.grad is None or not self.requires_grad:
                return
            self.grad = self.grad + out.grad / (self.data + 1e-12)

        out._parents = [self]
        out._backward = _backward
        return out

    def abs(self) -> "Tensor":
        out = Tensor(np.abs(self.data), self.requires_grad)

        def _backward() -> None:
            if out.grad is None or not self.requires_grad:
                return
            self.grad = self.grad + out.grad * np.sign(self.data)

        out._parents = [self]
        out._backward = _backward
        return out

    def norm(self) -> "Tensor":
        value = np.linalg.norm(self.data)
        out = Tensor(value, self.requires_grad)

        def _backward() -> None:
            if out.grad is None or not self.requires_grad:
                return
            if value == 0:
                return
            self.grad = self.grad + out.grad * self.data / value

        out._parents = [self]
        out._backward = _backward
        return out

    def __float__(self) -> float:
        return float(self.data)

    def item(self) -> float:
        return float(self.data)

    def __repr__(self) -> str:  # pragma: no cover - debug helper
        return f"Tensor(data={self.data}, requires_grad={self.requires_grad})"


def ensure_tensor(value: ArrayLike | Tensor) -> Tensor:
    if isinstance(value, Tensor):
        return value
    return Tensor(value, requires_grad=False)


def tensor(data: ArrayLike, device: device | str | None = None, requires_grad: bool = False) -> Tensor:
    return Tensor(data, requires_grad=requires_grad)


def linspace(start: float, end: float, steps: int, device: device | str | None = None) -> Tensor:
    return tensor(np.linspace(start, end, steps), device=device)


def diag(values: Tensor) -> Tensor:
    values = ensure_tensor(values)
    return Tensor(np.diag(values.data), requires_grad=values.requires_grad)


def sin(tensor_value: ArrayLike | Tensor) -> Tensor:
    return ensure_tensor(tensor_value).sin()


def exp(tensor_value: ArrayLike | Tensor) -> Tensor:
    return ensure_tensor(tensor_value).exp()


def sigmoid(tensor_value: ArrayLike | Tensor) -> Tensor:
    tensor_value = ensure_tensor(tensor_value)
    return 1.0 / (1.0 + (-tensor_value).exp())


def mean(tensor_value: Tensor, axis: Optional[int] = None) -> Tensor:
    return tensor_value.mean(axis=axis)


def sum(tensor_value: Tensor, axis: Optional[int] = None) -> Tensor:
    return tensor_value.sum(axis=axis)


def log(tensor_value: Tensor) -> Tensor:
    return tensor_value.log()


def abs(tensor_value: Tensor) -> Tensor:
    return tensor_value.abs()


def square(tensor_value: Tensor) -> Tensor:
    return tensor_value.square()


def softmax(tensor_value: Tensor, dim: int = 0) -> Tensor:
    max_val = tensor_value.data.max(axis=dim, keepdims=True)
    shifted = Tensor(tensor_value.data - max_val, requires_grad=tensor_value.requires_grad)
    exps = shifted.exp()
    sums = exps.sum(axis=dim, keepdims=True)
    return exps / sums


def zeros_like(tensor_value: Tensor) -> Tensor:
    return Tensor(np.zeros_like(tensor_value.data), requires_grad=tensor_value.requires_grad)


def ones_like(tensor_value: Tensor) -> Tensor:
    return Tensor(np.ones_like(tensor_value.data), requires_grad=tensor_value.requires_grad)


class linalg:  # pragma: no cover - thin wrapper
    @staticmethod
    def eigvalsh(tensor_value: Tensor) -> Tensor:
        data = ensure_tensor(tensor_value).data
        eigenvalues = np.linalg.eigvalsh(data)
        return Tensor(eigenvalues, requires_grad=False)


class nn:
    class Module:
        def __call__(self, *args, **kwargs):
            return self.forward(*args, **kwargs)

        def forward(self, *args, **kwargs):  # pragma: no cover - interface
            raise NotImplementedError

        def to(self, _device: device | str | None = None):
            return self

    class Linear(Module):
        def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None:
            rng = np.random.default_rng()
            self.weight = Tensor(rng.normal(scale=0.2, size=(out_features, in_features)))
            self.bias = Tensor(np.zeros(out_features)) if bias else None

        def forward(self, input_tensor: Tensor) -> Tensor:
            input_tensor = ensure_tensor(input_tensor)
            weighted = self.weight.data * input_tensor.data
            summed = weighted.sum(axis=1)
            result = Tensor(summed, requires_grad=input_tensor.requires_grad)

            def _backward() -> None:
                if result.grad is None:
                    return
                if input_tensor.requires_grad:
                    grad_input = (result.grad[:, None] * self.weight.data).sum(axis=0)
                    if input_tensor.grad is None:
                        input_tensor.grad = np.zeros_like(input_tensor.data)
                    input_tensor.grad = input_tensor.grad + grad_input

            result._parents = [input_tensor]
            result._backward = _backward
            if self.bias is not None:
                result = result + self.bias
            return result

    class init:  # pragma: no cover - compatibility shim
        @staticmethod
        def xavier_uniform_(_: Tensor) -> None:
            return None

        @staticmethod
        def zeros_(_: Tensor) -> None:
            return None


autograd = np  # placeholder to satisfy attribute access

__all__ = [
    "Tensor",
    "tensor",
    "linspace",
    "diag",
    "sin",
    "exp",
    "sigmoid",
    "mean",
    "sum",
    "log",
    "abs",
    "square",
    "softmax",
    "linalg",
    "nn",
    "device",
]